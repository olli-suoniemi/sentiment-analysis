{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/suonieo1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/suonieo1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the data from the file. Rename the columns to \"Text\" and \"Sentiment\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv', delimiter=',', encoding='latin-1', header=None)\n",
    "df = df.rename(columns=lambda x: ['Sentiment', 'Text'][x])\n",
    "df = df[['Text', 'Sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the preprocessing function. Here we tokenize the text, remove all punctuations and convert them to lowercase. Then we remove the stopwords and stem the words.\n",
    "#### Finally we join the words back into a single string and return the processed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenization (split the text into words)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove punctuation and convert to lowercase\n",
    "    words = [word.lower() for word in words if word.isalpha()]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Stemming (reducing words to their root form)\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # Join the cleaned words back into a single string\n",
    "    cleaned_text = ' '.join(words)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We apply the preprocessing to each text in the data, so the 'Text' column contains preprocessed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text Sentiment\n",
      "0  accord gran compani plan move product russia a...   neutral\n",
      "1  technopoli plan develop stage area less squar ...   neutral\n",
      "2  intern electron industri compani elcoteq laid ...  negative\n",
      "3  new product plant compani would increas capac ...  positive\n",
      "4  accord compani updat strategi year baswar targ...  positive\n"
     ]
    }
   ],
   "source": [
    "df['Text'] = df['Text'].apply(preprocess_text)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We split the data into training, validation and test sets.\n",
    "#### Then we further split the training data into training and validation sets using k-fold cross-validation.\n",
    "#### We are using method StratifiedKFold over the standard KFold. StratifiedKFold is often preferred over KFold in classification tasks, especially when you have imbalanced class distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df['Text']\n",
    "y = df['Sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, shuffle=True, random_state=42)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we extract features using feature engineering.\n",
    "\n",
    "#### First we initialize the TfidfVectorizer, Bag of Words (BoW) and Word2Vec vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000)  # TF-IDF with 10,000 features\n",
    "bow_vectorizer = CountVectorizer(max_features=10000)    # BoW with 10,000 features\n",
    "word2vec_model = Word2Vec(sentences=X_train, vector_size=100, window=5, min_count=1, sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The we create the lists to store the Tfidf, BoW and the Word2Vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_tfidf = []\n",
    "X_val_tfidf = []\n",
    "X_test_tfidf = []\n",
    "X_train_bow = []\n",
    "X_val_bow = []\n",
    "X_test_bow = []\n",
    "X_train_w2v = []\n",
    "X_val_w2v = []\n",
    "X_test_w2v = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for train_index, val_index in kfold.split(X, y):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    # Apply Tfidf vectorization\n",
    "    X_train_tfidf_fold = tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_val_tfidf_fold = tfidf_vectorizer.transform(X_val)\n",
    "    X_test_tfidf_fold = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "    X_train_tfidf.append(X_train_tfidf_fold)\n",
    "    X_val_tfidf.append(X_val_tfidf_fold)\n",
    "    X_test_tfidf.append(X_test_tfidf_fold)\n",
    "\n",
    "    # Apply BoW vectorization\n",
    "    X_train_bow_fold = bow_vectorizer.fit_transform(X_train)\n",
    "    X_val_bow_fold = bow_vectorizer.transform(X_val)\n",
    "    X_test_bow_fold = bow_vectorizer.transform(X_test)\n",
    "\n",
    "    X_train_bow.append(X_train_bow_fold)\n",
    "    X_val_bow.append(X_val_bow_fold)\n",
    "    X_test_bow.append(X_test_bow_fold)\n",
    "\n",
    "    # Initialize Word2Vec vectors\n",
    "    X_train_w2v_fold = []\n",
    "    X_val_w2v_fold = []\n",
    "    X_test_w2v_fold = []\n",
    "\n",
    "    for word in X_train:\n",
    "        if word in word2vec_model.wv:\n",
    "            X_train_w2v_fold.append(word2vec_model.wv[word])\n",
    "        else:\n",
    "            X_train_w2v_fold.append([0.0] * word2vec_model.vector_size)\n",
    "\n",
    "    for word in X_val:\n",
    "        if word in word2vec_model.wv:\n",
    "            X_val_w2v_fold.append(word2vec_model.wv[word])\n",
    "        else:\n",
    "            X_val_w2v_fold.append([0.0] * word2vec_model.vector_size)\n",
    "\n",
    "    for word in X_test:\n",
    "        if word in word2vec_model.wv:\n",
    "            X_test_w2v_fold.append(word2vec_model.wv[word])\n",
    "        else:\n",
    "            X_test_w2v_fold.append([0.0] * word2vec_model.vector_size)\n",
    "\n",
    "    X_train_w2v.append(X_train_w2v_fold)\n",
    "    X_val_w2v.append(X_val_w2v_fold)\n",
    "    X_test_w2v.append(X_test_w2v_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [4361, 4362]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Train and evaluate a model for Tfidf\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     lr_model_tfidf \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mlr_model_tfidf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_tfidf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     y_val_pred_lr_tfidf \u001b[38;5;241m=\u001b[39m lr_model_tfidf\u001b[38;5;241m.\u001b[39mpredict(X_val_tfidf[i])\n\u001b[1;32m      7\u001b[0m     y_test_pred_lr_tfidf \u001b[38;5;241m=\u001b[39m lr_model_tfidf\u001b[38;5;241m.\u001b[39mpredict(X_test_tfidf[i])\n",
      "File \u001b[0;32m/opt/software/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1196\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1194\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[0;32m-> 1196\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1204\u001b[0m check_classification_targets(y)\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[0;32m/opt/software/lib/python3.10/site-packages/sklearn/base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/software/lib/python3.10/site-packages/sklearn/utils/validation.py:1124\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1106\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m   1107\u001b[0m     X,\n\u001b[1;32m   1108\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1120\u001b[0m )\n\u001b[1;32m   1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m-> 1124\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m/opt/software/lib/python3.10/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [4361, 4362]"
     ]
    }
   ],
   "source": [
    "# Train and evaluate models for Tfidf, BoW, and Word2Vec vectors\n",
    "for i in range(10):\n",
    "    # Train and evaluate a model for Tfidf\n",
    "    lr_model_tfidf = LogisticRegression()\n",
    "    lr_model_tfidf.fit(X_train_tfidf[i], y_train)\n",
    "    y_val_pred_lr_tfidf = lr_model_tfidf.predict(X_val_tfidf[i])\n",
    "    y_test_pred_lr_tfidf = lr_model_tfidf.predict(X_test_tfidf[i])\n",
    "\n",
    "    # Train and evaluate a model for BoW\n",
    "    lr_model_bow = LogisticRegression()\n",
    "    lr_model_bow.fit(X_train_bow[i], y_train)\n",
    "    y_val_pred_lr_bow = lr_model_bow.predict(X_val_bow[i])\n",
    "    y_test_pred_lr_bow = lr_model_bow.predict(X_test_bow[i])\n",
    "\n",
    "    # Train and evaluate a model for Word2Vec\n",
    "    lr_model_w2v = LogisticRegression()\n",
    "    lr_model_w2v.fit(X_train_w2v[i], y_train)\n",
    "    y_val_pred_lr_w2v = lr_model_w2v.predict(X_val_w2v[i])\n",
    "    y_test_pred_lr_w2v = lr_model_w2v.predict(X_test_w2v[i])\n",
    "\n",
    "    # Print evaluation metrics for Tfidf, BoW, and Word2Vec\n",
    "    print(f\"Fold {i+1} - Tfidf Validation Results:\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_val, y_val_pred_lr_tfidf))\n",
    "    print(classification_report(y_val, y_val_pred_lr_tfidf))\n",
    "    print(f\"Fold {i+1} - Tfidf Test Results:\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_test_pred_lr_tfidf))\n",
    "    print(classification_report(y_test, y_test_pred_lr_tfidf))\n",
    "\n",
    "    print(f\"Fold {i+1} - BoW Validation Results:\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_val, y_val_pred_lr_bow))\n",
    "    print(classification_report(y_val, y_val_pred_lr_bow))\n",
    "    print(f\"Fold {i+1} - BoW Test Results:\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_test_pred_lr_bow))\n",
    "    print(classification_report(y_test, y_test_pred_lr_bow))\n",
    "\n",
    "    print(f\"Fold {i+1} - Word2Vec Validation Results:\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_val, y_val_pred_lr_w2v))\n",
    "    print(classification_report(y_val, y_val_pred_lr_w2v))\n",
    "    print(f\"Fold {i+1} - Word2Vec Test Results:\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_test_pred_lr_w2v))\n",
    "    print(classification_report(y_test, y_test_pred_lr_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
